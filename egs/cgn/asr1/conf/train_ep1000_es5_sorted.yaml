# minibatch related
batch-size: 128
maxlen-in: 800  # if input length  > maxlen_in, batchsize is automatically reduced
maxlen-out: 150 # if output length > maxlen_out, batchsize is automatically reduced
# optimization related
sortagrad: -1 # Feed samples from shortest to longest ; -1: enabled for all epochs, 0: disabled, other: enabled for 'other' epochs
opt: adadelta
epochs: 1000
patience: 5

# scheduled sampling option
sampling-probability: 0.0

# encoder related
etype: blstmp     # encoder architecture type
elayers: 4
eunits: 320
eprojs: 320
subsample: "1_2_2_2_1" # skip every n frame from input to nth layers
enc-init: exp/train_lstm_mtlalpha0.1_unigram_1000_data_all.2/results/model.loss.best

# decoder related
dlayers: 2
dunits: 300
decoder-dropout: 0.0

# attention related
atype: location
adim: 320
aconv-chans: 10
aconv-filts: 100
encoder-dropout: 0.0

# hybrid CTC/attention
mtlalpha: 0.0
